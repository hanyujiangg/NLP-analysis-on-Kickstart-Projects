{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content-based recommender for kickstarter projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rake_nltk in c:\\users\\raymond\\anaconda3\\lib\\site-packages (1.0.4)\n",
      "Requirement already satisfied: nltk in c:\\users\\raymond\\anaconda3\\lib\\site-packages (from rake_nltk) (3.5)\n",
      "Requirement already satisfied: tqdm in c:\\users\\raymond\\anaconda3\\lib\\site-packages (from nltk->rake_nltk) (4.50.2)\n",
      "Requirement already satisfied: joblib in c:\\users\\raymond\\anaconda3\\lib\\site-packages (from nltk->rake_nltk) (0.17.0)\n",
      "Requirement already satisfied: regex in c:\\users\\raymond\\anaconda3\\lib\\site-packages (from nltk->rake_nltk) (2020.10.15)\n",
      "Requirement already satisfied: click in c:\\users\\raymond\\anaconda3\\lib\\site-packages (from nltk->rake_nltk) (7.1.2)\n"
     ]
    }
   ],
   "source": [
    "# run this statement only once to install Rake\n",
    "!pip install rake_nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from rake_nltk import Rake\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import re, nltk, gensim\n",
    "from nltk.tokenize import ToktokTokenizer\n",
    "from nltk.stem import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Read in and analyse the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'student', 'design', 'build', 'studio', 'blurring', 'the', 'boundaries', 'of', 'architecture', 'and', 'ceramics', '(', 'and', 'birds', ').', 'A', 'patch', 'commemorating', 'the']\n",
      "['CLA', 'is', 'an', 'innovative', 'product', 'designed', 'to', 'provide', 'real', 'time', 'legal', 'resources', 'to', 'largely', 'underserved', 'individuals', '.', 'The', 'LightSpike', 'is']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>backers_count</th>\n",
       "      <th>blurb</th>\n",
       "      <th>category</th>\n",
       "      <th>converted_pledged_amount</th>\n",
       "      <th>country</th>\n",
       "      <th>country_displayable_name</th>\n",
       "      <th>created_at</th>\n",
       "      <th>creator</th>\n",
       "      <th>currency</th>\n",
       "      <th>currency_symbol</th>\n",
       "      <th>...</th>\n",
       "      <th>staff_pick</th>\n",
       "      <th>state</th>\n",
       "      <th>state_changed_at</th>\n",
       "      <th>static_usd_rate</th>\n",
       "      <th>urls</th>\n",
       "      <th>usd_pledged</th>\n",
       "      <th>usd_type</th>\n",
       "      <th>Final Main</th>\n",
       "      <th>blub</th>\n",
       "      <th>Unnamed: 38</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>27</td>\n",
       "      <td>A student design build studio blurring the bou...</td>\n",
       "      <td>sculpture art sculpture</td>\n",
       "      <td>1369</td>\n",
       "      <td>US</td>\n",
       "      <td>the United States</td>\n",
       "      <td>1334085321</td>\n",
       "      <td>{\"id\":1709491227,\"name\":\"Kevin Taylor\",\"is_reg...</td>\n",
       "      <td>USD</td>\n",
       "      <td>$</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>successful</td>\n",
       "      <td>1335762019</td>\n",
       "      <td>1.0</td>\n",
       "      <td>{\"web\":{\"project\":\"https://www.kickstarter.com...</td>\n",
       "      <td>1369.0</td>\n",
       "      <td>international</td>\n",
       "      <td>art</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>54</td>\n",
       "      <td>A patch commemorating the SN8 flight from Boca...</td>\n",
       "      <td>art art</td>\n",
       "      <td>1229</td>\n",
       "      <td>US</td>\n",
       "      <td>the United States</td>\n",
       "      <td>1602983631</td>\n",
       "      <td>{\"id\":1475027416,\"name\":\"Liem Bahneman\",\"slug\"...</td>\n",
       "      <td>USD</td>\n",
       "      <td>$</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>successful</td>\n",
       "      <td>1608170400</td>\n",
       "      <td>1.0</td>\n",
       "      <td>{\"web\":{\"project\":\"https://www.kickstarter.com...</td>\n",
       "      <td>1229.0</td>\n",
       "      <td>international</td>\n",
       "      <td>art</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>63</td>\n",
       "      <td>We are creating TRM movie posters for distribu...</td>\n",
       "      <td>digital art art digital art</td>\n",
       "      <td>1637</td>\n",
       "      <td>US</td>\n",
       "      <td>the United States</td>\n",
       "      <td>1341415158</td>\n",
       "      <td>{\"id\":80857009,\"name\":\"Erik van Ingen\",\"is_reg...</td>\n",
       "      <td>USD</td>\n",
       "      <td>$</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>successful</td>\n",
       "      <td>1342238341</td>\n",
       "      <td>1.0</td>\n",
       "      <td>{\"web\":{\"project\":\"https://www.kickstarter.com...</td>\n",
       "      <td>1637.5</td>\n",
       "      <td>international</td>\n",
       "      <td>art</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>26</td>\n",
       "      <td>A video performance piece for the self-designe...</td>\n",
       "      <td>performance art art performance art</td>\n",
       "      <td>2750</td>\n",
       "      <td>US</td>\n",
       "      <td>the United States</td>\n",
       "      <td>1307480670</td>\n",
       "      <td>{\"id\":744741310,\"name\":\"Melissa Basaran (delet...</td>\n",
       "      <td>USD</td>\n",
       "      <td>$</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>successful</td>\n",
       "      <td>1310767233</td>\n",
       "      <td>1.0</td>\n",
       "      <td>{\"web\":{\"project\":\"https://www.kickstarter.com...</td>\n",
       "      <td>2750.0</td>\n",
       "      <td>international</td>\n",
       "      <td>art</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>Providing young artists with creative sponsors...</td>\n",
       "      <td>installations art installations</td>\n",
       "      <td>75</td>\n",
       "      <td>US</td>\n",
       "      <td>the United States</td>\n",
       "      <td>1519066595</td>\n",
       "      <td>{\"id\":1773775377,\"name\":\"Chelsey Everest Eiel\"...</td>\n",
       "      <td>USD</td>\n",
       "      <td>$</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>failed</td>\n",
       "      <td>1522253573</td>\n",
       "      <td>1.0</td>\n",
       "      <td>{\"web\":{\"project\":\"https://www.kickstarter.com...</td>\n",
       "      <td>75.0</td>\n",
       "      <td>international</td>\n",
       "      <td>art</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   backers_count                                              blurb  \\\n",
       "0             27  A student design build studio blurring the bou...   \n",
       "1             54  A patch commemorating the SN8 flight from Boca...   \n",
       "2             63  We are creating TRM movie posters for distribu...   \n",
       "3             26  A video performance piece for the self-designe...   \n",
       "4              2  Providing young artists with creative sponsors...   \n",
       "\n",
       "                              category  converted_pledged_amount country  \\\n",
       "0              sculpture art sculpture                      1369      US   \n",
       "1                              art art                      1229      US   \n",
       "2          digital art art digital art                      1637      US   \n",
       "3  performance art art performance art                      2750      US   \n",
       "4      installations art installations                        75      US   \n",
       "\n",
       "  country_displayable_name  created_at  \\\n",
       "0        the United States  1334085321   \n",
       "1        the United States  1602983631   \n",
       "2        the United States  1341415158   \n",
       "3        the United States  1307480670   \n",
       "4        the United States  1519066595   \n",
       "\n",
       "                                             creator currency currency_symbol  \\\n",
       "0  {\"id\":1709491227,\"name\":\"Kevin Taylor\",\"is_reg...      USD               $   \n",
       "1  {\"id\":1475027416,\"name\":\"Liem Bahneman\",\"slug\"...      USD               $   \n",
       "2  {\"id\":80857009,\"name\":\"Erik van Ingen\",\"is_reg...      USD               $   \n",
       "3  {\"id\":744741310,\"name\":\"Melissa Basaran (delet...      USD               $   \n",
       "4  {\"id\":1773775377,\"name\":\"Chelsey Everest Eiel\"...      USD               $   \n",
       "\n",
       "   ...  staff_pick       state  state_changed_at  static_usd_rate  \\\n",
       "0  ...       False  successful        1335762019              1.0   \n",
       "1  ...       False  successful        1608170400              1.0   \n",
       "2  ...       False  successful        1342238341              1.0   \n",
       "3  ...       False  successful        1310767233              1.0   \n",
       "4  ...       False      failed        1522253573              1.0   \n",
       "\n",
       "                                                urls  usd_pledged  \\\n",
       "0  {\"web\":{\"project\":\"https://www.kickstarter.com...       1369.0   \n",
       "1  {\"web\":{\"project\":\"https://www.kickstarter.com...       1229.0   \n",
       "2  {\"web\":{\"project\":\"https://www.kickstarter.com...       1637.5   \n",
       "3  {\"web\":{\"project\":\"https://www.kickstarter.com...       2750.0   \n",
       "4  {\"web\":{\"project\":\"https://www.kickstarter.com...         75.0   \n",
       "\n",
       "        usd_type  Final Main blub  Unnamed: 38  \n",
       "0  international         art  NaN          NaN  \n",
       "1  international         art  NaN          NaN  \n",
       "2  international         art  NaN          NaN  \n",
       "3  international         art  NaN          NaN  \n",
       "4  international         art  NaN          NaN  \n",
       "\n",
       "[5 rows x 41 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "#Dataset\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "\n",
    "art = PlaintextCorpusReader('data/Train/Art', '.+\\.txt')\n",
    "tech = PlaintextCorpusReader('data/Train/Tech', '.+\\.txt')\n",
    "\n",
    "art_docs1 = [art.words(fid) for fid in art.fileids()]\n",
    "tech_docs1 = [tech.words(fid) for fid in tech.fileids()]\n",
    "\n",
    "print(art_docs1[0][0:20])\n",
    "print(tech_docs1[0][0:20])\n",
    "\n",
    "\n",
    "#DY dataset \n",
    "path = r'./data/' # use your path\n",
    "all_files = glob.glob(path + \"/*.csv\")\n",
    "\n",
    "li = []\n",
    "\n",
    "for filename in all_files:\n",
    "    df = pd.read_csv(filename, index_col=None, header=0)\n",
    "    li.append(df)\n",
    "\n",
    "frame = pd.concat(li, axis=0, ignore_index=True)\n",
    "\n",
    "# df = pd.read_csv('Kickstarter057.csv')\n",
    "\n",
    "df = frame\n",
    "df.head()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Processing data for DY\n",
    "def extract_cat(text):\n",
    "    text = text.split(\",\")\n",
    "    \n",
    "    text = text[1] + \" \" +text[2]\n",
    "    \n",
    "\n",
    "    text = text.replace (\"/\", \" \")\n",
    "    \n",
    "    \n",
    "    text = text.replace (\"name\", \"\")\n",
    "    text = text.replace (\"slug\", \"\")\n",
    "    \n",
    "    text = text.replace ('\"', \"\")\n",
    "    text = text.replace ('{', \"\")\n",
    "    text = text.replace (':', \"\")\n",
    "    \n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"\\'\\n\", \" \", text)\n",
    "    text = re.sub(r\"\\'\\xa0\", \" \", text)\n",
    "    text = re.sub('\\s+', ' ', text) # matches all whitespace characters\n",
    "    text = text.strip(' ')\n",
    "    return text\n",
    "\n",
    "\n",
    "\n",
    "df['category'] = df['category'].apply(lambda x: extract_cat(x))\n",
    "\n",
    "\n",
    "df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>category</th>\n",
       "      <th>blurb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bird Wall</td>\n",
       "      <td>sculpture art sculpture</td>\n",
       "      <td>A student design build studio blurring the bou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SpaceX SN8 Starship Prototype 12km Hop Patch</td>\n",
       "      <td>art art</td>\n",
       "      <td>A patch commemorating the SN8 flight from Boca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"The Real Maine\" movie poster</td>\n",
       "      <td>digital art art digital art</td>\n",
       "      <td>We are creating TRM movie posters for distribu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Voicing Islam Video Performance Piece</td>\n",
       "      <td>performance art art performance art</td>\n",
       "      <td>A video performance piece for the self-designe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Daughters of Revolution Project</td>\n",
       "      <td>installations art installations</td>\n",
       "      <td>Providing young artists with creative sponsors...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           name  \\\n",
       "0                                     Bird Wall   \n",
       "1  SpaceX SN8 Starship Prototype 12km Hop Patch   \n",
       "2                 \"The Real Maine\" movie poster   \n",
       "3         Voicing Islam Video Performance Piece   \n",
       "4           The Daughters of Revolution Project   \n",
       "\n",
       "                              category  \\\n",
       "0              sculpture art sculpture   \n",
       "1                              art art   \n",
       "2          digital art art digital art   \n",
       "3  performance art art performance art   \n",
       "4      installations art installations   \n",
       "\n",
       "                                               blurb  \n",
       "0  A student design build studio blurring the bou...  \n",
       "1  A patch commemorating the SN8 flight from Boca...  \n",
       "2  We are creating TRM movie posters for distribu...  \n",
       "3  A video performance piece for the self-designe...  \n",
       "4  Providing young artists with creative sponsors...  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df = df[['name','category','blurb']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "5\n",
      "Dictionary(11627 unique tokens: ['aalborg', 'ab', 'abaddon', 'abalon', 'abandon']...)\n",
      "<class 'list'>\n",
      "TECH\n"
     ]
    }
   ],
   "source": [
    "# Combine the categories of the corpus\n",
    "all_docs1 = art_docs1 + tech_docs1\n",
    "num_art_docs = len(art_docs1)\n",
    "print(num_art_docs)\n",
    "print (len(tech_docs1))\n",
    "\n",
    "# Processsing for stopwords, alphabetic words, Stemming \n",
    "\n",
    "all_docs2 = [[w.lower() for w in doc] for doc in all_docs1]\n",
    "\n",
    "import re\n",
    "all_docs3 = [[w for w in doc if re.search('^[a-z]+$',w)] for doc in all_docs2]\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop_list = stopwords.words('english')\n",
    "all_docs4 = [[w for w in doc if w not in stop_list] for doc in all_docs3]\n",
    "\n",
    "from nltk.stem.porter import *\n",
    "stemmer = PorterStemmer()\n",
    "all_docs5 = [[stemmer.stem(w) for w in doc] for doc in all_docs4]\n",
    "\n",
    "#Create dictionary\n",
    "from gensim import corpora\n",
    "dictionary = corpora.Dictionary(all_docs5)\n",
    "print(dictionary)\n",
    "\n",
    "# Convert all documents to TF Vectors\n",
    "all_tf_vectors = [dictionary.doc2bow(doc) for doc in all_docs5]\n",
    "\n",
    "#Label the trained data. Since the folder name is the label, I use the same labels.\n",
    "\n",
    "all_data_as_dict = [{id:1 for (id, tf_value) in vec} for vec in all_tf_vectors]\n",
    "print(type(all_data_as_dict))\n",
    "\n",
    "#print(all_data_as_dict)\n",
    "\n",
    "art_data = [(d, 'ART') for d in all_data_as_dict[0:num_art_docs]]\n",
    "tech_data = [(d, 'TECH') for d in all_data_as_dict[num_art_docs:]]\n",
    "all_labeled_data = art_data + tech_data\n",
    "\n",
    "#Generate the trained classifier\n",
    "classifier = nltk.NaiveBayesClassifier.train(all_labeled_data)\n",
    "\n",
    "test_doc = all_data_as_dict[4] #35 is crime article. Until 99 are all crime\n",
    "#print(all_data_as_dict[0])\n",
    "print(classifier.classify(test_doc))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Raymond/nltk_data'\n    - 'C:\\\\Users\\\\Raymond\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\Raymond\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\Raymond\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Raymond\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     82\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 83\u001b[1;33m                     \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"{}/{}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    584\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"\\n%s\\n%s\\n%s\\n\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 585\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    586\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet.zip/wordnet/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Raymond/nltk_data'\n    - 'C:\\\\Users\\\\Raymond\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\Raymond\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\Raymond\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Raymond\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-6711854fc072>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'blurb'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'blurb'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36mmap\u001b[1;34m(self, arg, na_action)\u001b[0m\n\u001b[0;32m   3968\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3969\u001b[0m         \"\"\"\n\u001b[1;32m-> 3970\u001b[1;33m         \u001b[0mnew_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_map_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mna_action\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3971\u001b[0m         return self._constructor(new_values, index=self.index).__finalize__(\n\u001b[0;32m   3972\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"map\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\base.py\u001b[0m in \u001b[0;36m_map_values\u001b[1;34m(self, mapper, na_action)\u001b[0m\n\u001b[0;32m   1158\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1159\u001b[0m         \u001b[1;31m# mapper is a function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1160\u001b[1;33m         \u001b[0mnew_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmap_f\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmapper\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1161\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1162\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mnew_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m<ipython-input-26-6711854fc072>\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(s)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'blurb'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'blurb'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-26-6711854fc072>\u001b[0m in \u001b[0;36mpreprocess\u001b[1;34m(sentence)\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[0mfiltered_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mw\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtokens\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'english'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[0mstem_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstemmer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfiltered_words\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m     \u001b[0mlemma_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlemmatizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstem_words\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;34m\" \"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_words\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-26-6711854fc072>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[0mfiltered_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mw\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtokens\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'english'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[0mstem_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstemmer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfiltered_words\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m     \u001b[0mlemma_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlemmatizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstem_words\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;34m\" \"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_words\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\stem\\wordnet.py\u001b[0m in \u001b[0;36mlemmatize\u001b[1;34m(self, word, pos)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mlemmatize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNOUN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m         \u001b[0mlemmas\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwordnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_morphy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlemmas\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mlemmas\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    118\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 120\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    121\u001b[0m         \u001b[1;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m         \u001b[1;31m# __class__ to something new:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     83\u001b[0m                     \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"{}/{}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 85\u001b[1;33m                     \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m         \u001b[1;31m# Load the corpus.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     78\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 80\u001b[1;33m                 \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"{}/{}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     81\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    583\u001b[0m     \u001b[0msep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"*\"\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m70\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    584\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"\\n%s\\n%s\\n%s\\n\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 585\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    586\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    587\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Raymond/nltk_data'\n    - 'C:\\\\Users\\\\Raymond\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\Raymond\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\Raymond\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Raymond\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer,PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer() \n",
    "\n",
    "def preprocess(sentence):\n",
    "    sentence=str(sentence)\n",
    "    sentence = sentence.lower()\n",
    "    sentence=sentence.replace('{html}',\"\") \n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, '', sentence)\n",
    "    rem_url=re.sub(r'http\\S+', '',cleantext)\n",
    "    rem_num = re.sub('[0-9]+', '', rem_url)\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(rem_num)  \n",
    "    filtered_words = [w for w in tokens if len(w) > 2 if not w in stopwords.words('english')]\n",
    "    stem_words=[stemmer.stem(w) for w in filtered_words]\n",
    "    lemma_words=[lemmatizer.lemmatize(w) for w in stem_words]\n",
    "    return \" \".join(filtered_words)\n",
    "\n",
    "\n",
    "df['blurb']=df['blurb'].map(lambda s:preprocess(s)) \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfcat = df['category']\n",
    "\n",
    "dfcat.head \n",
    "\n",
    "num_dfcat = len(dfcat)\n",
    "\n",
    "#print(num_dfcat)\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "dfcat = dfcat.apply(word_tokenize)\n",
    "dfcat.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "\n",
    "dictionary = corpora.Dictionary(dfcat)\n",
    "\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Validate \n",
    "\n",
    "# Read the files in validate folder and preparing the validation corpus\n",
    "art_validation = PlaintextCorpusReader('data/Validate/Art', '.+\\.txt')\n",
    "tech_validation = PlaintextCorpusReader('data/Validate/Tech', '.+\\.txt')\n",
    "comics_validation = PlaintextCorpusReader('data/Validate/Comics', '.+\\.txt')\n",
    "film_validation = PlaintextCorpusReader('data/Validate/Film', '.+\\.txt')\n",
    "music_validation = PlaintextCorpusReader('data/Validate/Music', '.+\\.txt')\n",
    "photography_validation = PlaintextCorpusReader('data/Validate/Photography', '.+\\.txt')\n",
    "publishing_validation = PlaintextCorpusReader('data/Validate/Publishing', '.+\\.txt')\n",
    "\n",
    "\n",
    "# Tokenization\n",
    "art_valid_docs1 = [art_validation.words(fid) for fid in art_validation.fileids()]\n",
    "tech_valid_docs1 = [tech_validation.words(fid) for fid in tech_validation.fileids()]\n",
    "comics_valid_docs1 = [comics_validation.words(fid) for fid in comics_validation.fileids()]\n",
    "film_valid_docs1 = [film_validation.words(fid) for fid in film_validation.fileids()]\n",
    "music_valid_docs1 = [music_validation.words(fid) for fid in music_validation.fileids()]\n",
    "photography_valid_docs1 = [photography_validation.words(fid) for fid in photography_validation.fileids()]\n",
    "publishing_valid_docs1 = [publishing_validation.words(fid) for fid in publishing_validation.fileids()]\n",
    "\n",
    "\n",
    "# Combine the two sets of documents for easy processing.\n",
    "all_valid_docs = art_valid_docs1 + tech_valid_docs1 + comics_valid_docs1 + film_valid_docs1 + music_valid_docs1 + photography_valid_docs1 + publishing_valid_docs1\n",
    "\n",
    "\n",
    "# This number will be used to separate the two sets of documents later.\n",
    "num_art_valid_docs = len(art_valid_docs1)\n",
    "\n",
    "# Text pre-processing, including stop word removal, stemming, etc.\n",
    "all_valid_docs2 = [[w.lower() for w in doc] for doc in all_valid_docs]\n",
    "all_valid_docs3 = [[w for w in doc if re.search('^[a-z]+$',w)] for doc in all_valid_docs2]\n",
    "all_valid_docs4 = [[w for w in doc if w not in stop_list] for doc in all_valid_docs3]\n",
    "all_valid_docs5 = [[stemmer.stem(w) for w in doc] for doc in all_valid_docs4]\n",
    "\n",
    "# Note that we're using the dictionary created earlier.\n",
    "all_valid_tf_vectors = [dictionary.doc2bow(doc) for doc in all_valid_docs5]\n",
    "\n",
    "# Convert documents into dict representation.\n",
    "all_valid_data_as_dict = [{id:1 for (id, tf_value) in vec} for vec in all_valid_tf_vectors]\n",
    "\n",
    "# Separate the two sets of documents and add labels.\n",
    "art_valid_data_with_labels = [(d, 'art') for d in all_valid_data_as_dict[0:num_art_valid_docs]]\n",
    "tech_valid_data_with_labels  = [(d, 'tech') for d in all_valid_data_as_dict[num_art_valid_docs:2]]\n",
    "comics_valid_data_with_labels  = [(d, 'comics') for d in all_valid_data_as_dict[3:4]]\n",
    "film_valid_data_with_labels  = [(d, 'film') for d in all_valid_data_as_dict[4:5]]\n",
    "music_valid_data_with_labels  = [(d, 'music') for d in all_valid_data_as_dict[5:6]]\n",
    "photography_valid_data_with_labels  = [(d, 'photography') for d in all_valid_data_as_dict[6:7]]\n",
    "publishing_valid_data_with_labels  = [(d, 'publishing') for d in all_valid_data_as_dict[7:8]]\n",
    "\n",
    "\n",
    "\n",
    "# Combine the labeled documents.\n",
    "all_valid_data_with_labels = art_valid_data_with_labels + tech_valid_data_with_labels + comics_valid_data_with_labels + film_valid_data_with_labels + music_valid_data_with_labels + photography_valid_data_with_labels + publishing_valid_data_with_labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Basically preprocessing date from dataset\n",
    "\n",
    "# Combine the categories of the corpus\n",
    "all_docs1 = art_docs1 + tech_docs1 + comics_docs1 + film_docs1 + music_docs1 + photography_docs1 + publishing_docs1\n",
    "num_art_docs = len(art_docs1)\n",
    "print(num_art_docs)\n",
    "print (len(tech_docs1))\n",
    "\n",
    "# Processsing for stopwords, alphabetic words, Stemming \n",
    "\n",
    "all_docs2 = [[w.lower() for w in doc] for doc in all_docs1]\n",
    "\n",
    "import re\n",
    "all_docs3 = [[w for w in doc if re.search('^[a-z]+$',w)] for doc in all_docs2]\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop_list = stopwords.words('english')\n",
    "all_docs4 = [[w for w in doc if w not in stop_list] for doc in all_docs3]\n",
    "\n",
    "from nltk.stem.porter import *\n",
    "stemmer = PorterStemmer()\n",
    "all_docs5 = [[stemmer.stem(w) for w in doc] for doc in all_docs4]\n",
    "\n",
    "#Create dictionary\n",
    "from gensim import corpora\n",
    "dictionary = corpora.Dictionary(all_docs5)\n",
    "print(dictionary)\n",
    "\n",
    "# Convert all documents to TF Vectors\n",
    "all_tf_vectors = [dictionary.doc2bow(doc) for doc in all_docs5]\n",
    "\n",
    "#Label the trained data. Since the folder name is the label, I use the same labels.\n",
    "\n",
    "all_data_as_dict = [{id:1 for (id, tf_value) in vec} for vec in all_tf_vectors]\n",
    "print(type(all_data_as_dict))\n",
    "\n",
    "\n",
    "#print(all_data_as_dict). The labels are generated by our own dataset and used here.\n",
    "art_data = [(d, 'art') for d in all_data_as_dict[0:num_art_docs]] #First document to number of art documents, which is 4. Document 0-4\n",
    "tech_data = [(d, 'tech') for d in all_data_as_dict[num_art_docs:2]]\n",
    "comics_data = [(d, 'comics') for d in all_data_as_dict[3:4]]\n",
    "film_data = [(d, 'film') for d in all_data_as_dict[4:5]]\n",
    "music_data = [(d, 'music') for d in all_data_as_dict[5:6]]\n",
    "photography_data = [(d, 'photography') for d in all_data_as_dict[6:7]]\n",
    "publishing_data = [(d, 'publishing') for d in all_data_as_dict[7:8]]\n",
    "\n",
    "\n",
    "all_labeled_data = art_data + tech_data + comics_data + film_data + music_data + photography_data + publishing_data\n",
    "\n",
    "#Generate the trained classifier\n",
    "classifier = nltk.NaiveBayesClassifier.train(all_labeled_data)\n",
    "\n",
    "test_doc = all_data_as_dict[6]\n",
    "#print(all_data_as_dict[0])\n",
    "print(classifier.classify(test_doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load dataset\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "#Dataset\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "\n",
    "art = PlaintextCorpusReader('data/Train/Art', '.+\\.txt')\n",
    "tech = PlaintextCorpusReader('data/Train/Tech', '.+\\.txt')\n",
    "comics = PlaintextCorpusReader('data/Train/Comics', '.+\\.txt')\n",
    "film = PlaintextCorpusReader('data/Train/Film', '.+\\.txt')\n",
    "music = PlaintextCorpusReader('data/Train/Music', '.+\\.txt')\n",
    "photography = PlaintextCorpusReader('data/Train/Photography', '.+\\.txt')\n",
    "publishing = PlaintextCorpusReader('data/Train/Publishing', '.+\\.txt')\n",
    "\n",
    "art_docs1 = [art.words(fid) for fid in art.fileids()]\n",
    "tech_docs1 = [tech.words(fid) for fid in tech.fileids()]\n",
    "comics_docs1 = [comics.words(fid) for fid in comics.fileids()]\n",
    "film_docs1 = [film.words(fid) for fid in film.fileids()]\n",
    "music_docs1 = [music.words(fid) for fid in music.fileids()]\n",
    "photography_docs1 = [photography.words(fid) for fid in photography.fileids()]\n",
    "publishing_docs1 = [publishing.words(fid) for fid in publishing.fileids()]\n",
    "\n",
    "print(art_docs1[0][0:20])\n",
    "print(tech_docs1[0][0:20])\n",
    "print(comics_docs1[0][0:20])\n",
    "print(film_docs1[0][0:20])\n",
    "print(music_docs1[0][0:20])\n",
    "print(photography_docs1[0][0:20])\n",
    "print(publishing_docs1[0][0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Basically preprocessing date from dataset\n",
    "\n",
    "# Combine the categories of the corpus\n",
    "all_docs1 = art_docs1 + tech_docs1 + comics_docs1 + film_docs1 + music_docs1 + photography_docs1 + publishing_docs1\n",
    "num_art_docs = len(art_docs1)\n",
    "num_tech_docs = len(tech_docs1)\n",
    "num_comics_docs = len(comics_docs1)\n",
    "num_film_docs = len(film_docs1)\n",
    "num_music_docs = len(music_docs1)\n",
    "num_photography_docs = len(photography_docs1)\n",
    "num_publishing_docs = len(publishing_docs1)\n",
    "\n",
    "print(num_art_docs)\n",
    "print(num_tech_docs)\n",
    "print(num_comics_docs)\n",
    "print(num_film_docs)\n",
    "print(num_music_docs)\n",
    "print(num_photography_docs)\n",
    "print(num_publishing_docs)\n",
    "\n",
    "\n",
    "# Processsing for stopwords, alphabetic words, Stemming \n",
    "\n",
    "all_docs2 = [[w.lower() for w in doc] for doc in all_docs1]\n",
    "\n",
    "import re\n",
    "all_docs3 = [[w for w in doc if re.search('^[a-z]+$',w)] for doc in all_docs2]\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop_list = stopwords.words('english')\n",
    "all_docs4 = [[w for w in doc if w not in stop_list] for doc in all_docs3]\n",
    "\n",
    "from nltk.stem.porter import *\n",
    "stemmer = PorterStemmer()\n",
    "all_docs5 = [[stemmer.stem(w) for w in doc] for doc in all_docs4]\n",
    "\n",
    "#Create dictionary\n",
    "from gensim import corpora\n",
    "dictionary = corpora.Dictionary(all_docs5)\n",
    "print(dictionary)\n",
    "\n",
    "# Convert all documents to TF Vectors\n",
    "all_tf_vectors = [dictionary.doc2bow(doc) for doc in all_docs5]\n",
    "\n",
    "#Label the trained data. Since the folder name is the label, I use the same labels.\n",
    "\n",
    "all_data_as_dict = [{id:1 for (id, tf_value) in vec} for vec in all_tf_vectors]\n",
    "print(type(all_data_as_dict))\n",
    "\n",
    "\n",
    "#print(all_data_as_dict). The labels are generated by our own dataset and used here.\n",
    "art_data = [(d, 'art') for d in all_data_as_dict[0:num_art_docs]] #First document to number of art documents, which is 4. Document 0-4\n",
    "tech_data = [(d, 'tech') for d in all_data_as_dict[num_art_docs:num_tech_docs]]\n",
    "comics_data = [(d, 'comics') for d in all_data_as_dict[num_tech_docs:num_comics_docs]]\n",
    "film_data = [(d, 'film') for d in all_data_as_dict[num_comics_docs:num_film_docs]]\n",
    "music_data = [(d, 'music') for d in all_data_as_dict[num_film_docs:num_music_docs]]\n",
    "photography_data = [(d, 'photography') for d in all_data_as_dict[num_music_docs:num_photography_docs]]\n",
    "publishing_data = [(d, 'publishing') for d in all_data_as_dict[num_photography_docs:]]\n",
    "\n",
    "\n",
    "all_labeled_data = art_data + tech_data + comics_data + film_data + music_data + photography_data + publishing_data\n",
    "\n",
    "#Generate the trained classifier\n",
    "classifier = nltk.NaiveBayesClassifier.train(all_labeled_data)\n",
    "\n",
    "test_doc = all_data_as_dict[1]\n",
    "#print(all_data_as_dict[0])\n",
    "print(classifier.classify(test_doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Validate \n",
    "\n",
    "# Read the files in validate folder and preparing the validation corpus\n",
    "art_validation = PlaintextCorpusReader('data/Validate/Art', '.+\\.txt')\n",
    "tech_validation = PlaintextCorpusReader('data/Validate/Tech', '.+\\.txt')\n",
    "\n",
    "\n",
    "# Tokenization\n",
    "art_valid_docs1 = [art_validation.words(fid) for fid in art_validation.fileids()]\n",
    "tech_valid_docs1 = [tech_validation.words(fid) for fid in tech_validation.fileids()]\n",
    "\n",
    "\n",
    "# Combine the two sets of documents for easy processing.\n",
    "all_valid_docs = art_valid_docs1 + tech_valid_docs1\n",
    "\n",
    "\n",
    "# This number will be used to separate the two sets of documents later.\n",
    "num_art_valid_docs = len(art_valid_docs1)\n",
    "\n",
    "# Text pre-processing, including stop word removal, stemming, etc.\n",
    "all_valid_docs2 = [[w.lower() for w in doc] for doc in all_valid_docs]\n",
    "all_valid_docs3 = [[w for w in doc if re.search('^[a-z]+$',w)] for doc in all_valid_docs2]\n",
    "all_valid_docs4 = [[w for w in doc if w not in stop_list] for doc in all_valid_docs3]\n",
    "all_valid_docs5 = [[stemmer.stem(w) for w in doc] for doc in all_valid_docs4]\n",
    "\n",
    "# Note that we're using the dictionary created earlier.\n",
    "all_valid_tf_vectors = [dictionary.doc2bow(doc) for doc in all_valid_docs5]\n",
    "\n",
    "# Convert documents into dict representation.\n",
    "all_valid_data_as_dict = [{id:1 for (id, tf_value) in vec} for vec in all_valid_tf_vectors]\n",
    "\n",
    "# Separate the two sets of documents and add labels.\n",
    "art_valid_data_with_labels = [(d, 'art') for d in all_valid_data_as_dict[0:num_art_valid_docs]]\n",
    "tech_valid_data_with_labels  = [(d, 'tech') for d in all_valid_data_as_dict[num_art_valid_docs:]]\n",
    "\n",
    "\n",
    "# Combine the labeled documents.\n",
    "all_valid_data_with_labels = art_valid_data_with_labels + tech_valid_data_with_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Basically preprocessing date from dataset\n",
    "\n",
    "# Combine the categories of the corpus\n",
    "all_docs1 = art_docs1 + tech_docs1 + comics_docs1 + film_docs1 + music_docs1 + photography_docs1 + publishing_docs1\n",
    "num_art_docs = len(art_docs1)\n",
    "num_2 = len(art_docs1) + len(tech_docs1)\n",
    "num_3 = num_2 + len(comics_docs1)\n",
    "num_4 = num_3 + len(film_docs1)\n",
    "num_5 = num_4 + len(music_docs1)\n",
    "num_6 = num_5 + len(photography_docs1)\n",
    "\n",
    "#For verifying the whether the output in dictionary is correct\n",
    "print(num_art_docs)\n",
    "print (len(tech_docs1))\n",
    "print (len(comics_docs1))\n",
    "print (len(film_docs1))\n",
    "print (len(music_docs1))\n",
    "print (len(photography_docs1))\n",
    "\n",
    "# Processsing for stopwords, alphabetic words, Stemming \n",
    "\n",
    "all_docs2 = [[w.lower() for w in doc] for doc in all_docs1]\n",
    "\n",
    "import re\n",
    "all_docs3 = [[w for w in doc if re.search('^[a-z]+$',w)] for doc in all_docs2]\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop_list = stopwords.words('english')\n",
    "all_docs4 = [[w for w in doc if w not in stop_list] for doc in all_docs3]\n",
    "\n",
    "from nltk.stem.porter import *\n",
    "stemmer = PorterStemmer()\n",
    "all_docs5 = [[stemmer.stem(w) for w in doc] for doc in all_docs4]\n",
    "\n",
    "#Create dictionary\n",
    "from gensim import corpora\n",
    "dictionary = corpora.Dictionary(all_docs5)\n",
    "print(dictionary)\n",
    "\n",
    "# Convert all documents to TF Vectors\n",
    "all_tf_vectors = [dictionary.doc2bow(doc) for doc in all_docs5]\n",
    "\n",
    "#Label the trained data. Since the folder name is the label, I use the same labels.\n",
    "\n",
    "all_data_as_dict = [{id:1 for (id, tf_value) in vec} for vec in all_tf_vectors]\n",
    "print(type(all_data_as_dict))\n",
    "\n",
    "\n",
    "#print(all_data_as_dict). The labels are generated by our own dataset and used here.\n",
    "art_data = [(d, 'art') for d in all_data_as_dict[0:num_art_docs]] #First document to number of art documents, which is 4. Document 0-4\n",
    "tech_data = [(d, 'tech') for d in all_data_as_dict[num_art_docs:num_2]]\n",
    "comics_data = [(d, 'comics') for d in all_data_as_dict[num_2:num_3]]\n",
    "film_data = [(d, 'film') for d in all_data_as_dict[num_3:num_4]]\n",
    "music_data = [(d, 'music') for d in all_data_as_dict[num_4:num_5]]\n",
    "photography_data = [(d, 'photography') for d in all_data_as_dict[num_5:num_6]]\n",
    "publishing_data = [(d, 'publishing') for d in all_data_as_dict[num_6:]]\n",
    "\n",
    "all_labeled_data = art_data + tech_data + comics_data + film_data + music_data + photography_data + publishing_data\n",
    "\n",
    "#Generate the trained classifier\n",
    "classifier = nltk.NaiveBayesClassifier.train(all_labeled_data)\n",
    "\n",
    "test_doc = all_data_as_dict[699]\n",
    "#print(all_data_as_dict[0])\n",
    "print(classifier.classify(test_doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Validate \n",
    "\n",
    "# Read the files in validate folder and preparing the validation corpus\n",
    "art_validation = PlaintextCorpusReader('data/Validate/Art', '.+\\.txt')\n",
    "tech_validation = PlaintextCorpusReader('data/Validate/Tech', '.+\\.txt')\n",
    "comics_validation = PlaintextCorpusReader('data/Validate/Comics', '.+\\.txt')\n",
    "film_validation = PlaintextCorpusReader('data/Validate/Film', '.+\\.txt')\n",
    "music_validation = PlaintextCorpusReader('data/Validate/Music', '.+\\.txt')\n",
    "photography_validation = PlaintextCorpusReader('data/Validate/Photography', '.+\\.txt')\n",
    "publishing_validation = PlaintextCorpusReader('data/Validate/Publishing', '.+\\.txt')\n",
    "\n",
    "\n",
    "# Tokenization\n",
    "art_valid_docs1 = [art_validation.words(fid) for fid in art_validation.fileids()]\n",
    "tech_valid_docs1 = [tech_validation.words(fid) for fid in tech_validation.fileids()]\n",
    "comics_valid_docs1 = [comics_validation.words(fid) for fid in comics_validation.fileids()]\n",
    "film_valid_docs1 = [film_validation.words(fid) for fid in film_validation.fileids()]\n",
    "music_valid_docs1 = [music_validation.words(fid) for fid in music_validation.fileids()]\n",
    "photography_valid_docs1 = [photography_validation.words(fid) for fid in photography_validation.fileids()]\n",
    "publishing_valid_docs1 = [publishing_validation.words(fid) for fid in publishing_validation.fileids()]\n",
    "\n",
    "\n",
    "# Combine the two sets of documents for easy processing.\n",
    "all_valid_docs = art_valid_docs1 + tech_valid_docs1 + comics_valid_docs1 + film_valid_docs1 + music_valid_docs1 + photography_valid_docs1 + publishing_valid_docs1\n",
    "\n",
    "\n",
    "# This number will be used to separate the two sets of documents later.\n",
    "num_art_valid_docs = len(art_valid_docs1)\n",
    "num_valid_2 = num_art_valid_docs + len(tech_valid_docs1)\n",
    "num_valid_3 = num_2 + len(comics_valid_docs1)\n",
    "num_valid_4 = num_3 + len(film_valid_docs1)\n",
    "num_valid_5 = num_4 + len(music_valid_docs1)\n",
    "num_valid_6 = num_5 + len(photography_valid_docs1)\n",
    "                    \n",
    "\n",
    "# Text pre-processing, including stop word removal, stemming, etc.\n",
    "all_valid_docs2 = [[w.lower() for w in doc] for doc in all_valid_docs]\n",
    "all_valid_docs3 = [[w for w in doc if re.search('^[a-z]+$',w)] for doc in all_valid_docs2]\n",
    "all_valid_docs4 = [[w for w in doc if w not in stop_list] for doc in all_valid_docs3]\n",
    "all_valid_docs5 = [[stemmer.stem(w) for w in doc] for doc in all_valid_docs4]\n",
    "\n",
    "# Note that we're using the dictionary created earlier.\n",
    "all_valid_tf_vectors = [dictionary.doc2bow(doc) for doc in all_valid_docs5]\n",
    "\n",
    "# Convert documents into dict representation.\n",
    "all_valid_data_as_dict = [{id:1 for (id, tf_value) in vec} for vec in all_valid_tf_vectors]\n",
    "\n",
    "# Separate the two sets of documents and add labels.\n",
    "art_valid_data_with_labels = [(d, 'art') for d in all_valid_data_as_dict[0:num_art_valid_docs]]\n",
    "tech_valid_data_with_labels  = [(d, 'tech') for d in all_valid_data_as_dict[num_art_valid_docs:num_valid_2]]\n",
    "comics_valid_data_with_labels  = [(d, 'comics') for d in all_valid_data_as_dict[num_valid_2:num_valid_3]]\n",
    "film_valid_data_with_labels  = [(d, 'film') for d in all_valid_data_as_dict[num_valid_3:num_valid_4]]\n",
    "music_valid_data_with_labels  = [(d, 'music') for d in all_valid_data_as_dict[num_valid_4:num_valid_5]]\n",
    "photography_valid_data_with_labels  = [(d, 'photography') for d in all_valid_data_as_dict[num_valid_5:num_valid_6]]\n",
    "publishing_valid_data_with_labels  = [(d, 'publishing') for d in all_valid_data_as_dict[num_valid_2:]]\n",
    "\n",
    "\n",
    "# Combine the labeled documents.\n",
    "all_valid_data_with_labels = art_valid_data_with_labels + tech_valid_data_with_labels + comics_valid_data_with_labels + film_valid_data_with_labels + music_valid_data_with_labels + photography_valid_data_with_labels + publishing_valid_data_with_labels "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
